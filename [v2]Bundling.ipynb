{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded auctions file with auction_id to location mapping.\n",
      "Created pickupdates dictionary for pickupdate to auction_ID mapping.\n",
      "Created pickupdates dictionary for auction_ID to pickupdate mapping.\n",
      "\n",
      "--- Processing '000002.csv' ---\n",
      "\n",
      "--- Processing '000003.csv' ---\n",
      "\n",
      "--- Processing '000004.csv' ---\n",
      "\n",
      "--- Processing '000005.csv' ---\n",
      "\n",
      "--- Processing '000006.csv' ---\n",
      "\n",
      "--- Processing '000007.csv' ---\n",
      "\n",
      "--- Processing '000008.csv' ---\n",
      "\n",
      "--- Processing '000009.csv' ---\n",
      "\n",
      "--- Processing '000010.csv' ---\n",
      "\n",
      "--- Processing '000011.csv' ---\n",
      "\n",
      "--- Processing '000012.csv' ---\n",
      "\n",
      "--- Processing '000016.csv' ---\n",
      "\n",
      "--- Processing '000017.csv' ---\n",
      "\n",
      "--- Processing '000018.csv' ---\n",
      "\n",
      "--- Processing '000019.csv' ---\n",
      "\n",
      "--- Processing '000020.csv' ---\n",
      "\n",
      "--- Processing '000021.csv' ---\n",
      "\n",
      "--- Processing '000022.csv' ---\n",
      "\n",
      "--- Processing '000023.csv' ---\n",
      "\n",
      "--- Processing '000032.csv' ---\n",
      "\n",
      "--- Processing '000033.csv' ---\n",
      "\n",
      "--- Processing '000034.csv' ---\n",
      "\n",
      "--- Processing '000035.csv' ---\n",
      "\n",
      "--- Processing '000042.csv' ---\n",
      "\n",
      "--- Processing '000043.csv' ---\n",
      "\n",
      "--- Processing '000044.csv' ---\n",
      "\n",
      "--- Processing '000045.csv' ---\n",
      "\n",
      "--- Processing '000046.csv' ---\n",
      "\n",
      "--- Processing '000047.csv' ---\n",
      "\n",
      "--- Processing '000048.csv' ---\n",
      "\n",
      "--- Processing '000049.csv' ---\n",
      "\n",
      "--- Processing '000050.csv' ---\n",
      "\n",
      "--- Processing '000053.csv' ---\n",
      "\n",
      "--- Processing '000054.csv' ---\n",
      "\n",
      "--- Processing '000055.csv' ---\n",
      "\n",
      "--- Processing '000056.csv' ---\n",
      "\n",
      "--- Processing '000057.csv' ---\n",
      "\n",
      "--- Processing '000058.csv' ---\n",
      "\n",
      "--- Processing '000059.csv' ---\n",
      "\n",
      "--- Processing '000060.csv' ---\n",
      "\n",
      "--- Processing '000066.csv' ---\n",
      "\n",
      "--- Processing '000067.csv' ---\n",
      "\n",
      "--- Processing '000068.csv' ---\n",
      "\n",
      "--- Processing '000069.csv' ---\n",
      "\n",
      "--- Processing '000070.csv' ---\n",
      "\n",
      "--- Processing '000071.csv' ---\n",
      "\n",
      "--- Processing '000072.csv' ---\n",
      "\n",
      "--- Processing '000073.csv' ---\n",
      "\n",
      "--- Processing '000074.csv' ---\n",
      "\n",
      "--- Processing '000075.csv' ---\n",
      "\n",
      "--- Processing '000076.csv' ---\n",
      "\n",
      "--- Processing '000077.csv' ---\n",
      "\n",
      "--- Processing '000078.csv' ---\n",
      "\n",
      "--- Processing '000079.csv' ---\n",
      "\n",
      "--- Processing '000080.csv' ---\n",
      "\n",
      "--- Processing '000081.csv' ---\n",
      "\n",
      "--- Processing '000082.csv' ---\n",
      "\n",
      "--- Processing '000083.csv' ---\n",
      "\n",
      "--- Processing '000084.csv' ---\n",
      "\n",
      "--- Processing '000085.csv' ---\n",
      "\n",
      "--- Processing '000086.csv' ---\n",
      "\n",
      "--- Processing '000087.csv' ---\n",
      "\n",
      "--- Processing '000088.csv' ---\n",
      "\n",
      "--- Processing '000089.csv' ---\n",
      "\n",
      "--- Processing '000090.csv' ---\n",
      "\n",
      "--- Processing '000091.csv' ---\n",
      "\n",
      "--- Processing '000092.csv' ---\n",
      "\n",
      "--- Processing '000093.csv' ---\n",
      "\n",
      "--- Processing '000094.csv' ---\n",
      "\n",
      "--- Processing '000095.csv' ---\n",
      "\n",
      "--- Processing '000096.csv' ---\n",
      "\n",
      "--- Processing '000097.csv' ---\n",
      "\n",
      "--- Processing '000098.csv' ---\n",
      "\n",
      "--- Processing '000099.csv' ---\n",
      "\n",
      "--- Processing '000101.csv' ---\n",
      "\n",
      "--- Processing '000102.csv' ---\n",
      "\n",
      "--- Processing '000103.csv' ---\n",
      "\n",
      "--- Processing '000104.csv' ---\n",
      "\n",
      "--- Processing '000105.csv' ---\n",
      "\n",
      "--- Processing '000106.csv' ---\n",
      "\n",
      "--- Processing '000107.csv' ---\n",
      "\n",
      "--- Processing '000108.csv' ---\n",
      "\n",
      "--- Processing '000109.csv' ---\n",
      "\n",
      "--- Processing '000110.csv' ---\n",
      "\n",
      "--- Processing '000111.csv' ---\n",
      "\n",
      "--- Processing '000112.csv' ---\n",
      "\n",
      "--- Processing '000113.csv' ---\n",
      "\n",
      "--- Processing '000114.csv' ---\n",
      "\n",
      "--- Processing '000115.csv' ---\n",
      "\n",
      "--- Processing '000116.csv' ---\n",
      "\n",
      "--- Processing '000117.csv' ---\n",
      "\n",
      "--- Processing '000118.csv' ---\n",
      "\n",
      "--- Processing '000119.csv' ---\n",
      "\n",
      "--- Processing '000120.csv' ---\n",
      "\n",
      "--- Processing '000121.csv' ---\n",
      "\n",
      "--- Processing '000122.csv' ---\n",
      "\n",
      "--- Processing '000123.csv' ---\n",
      "\n",
      "--- Processing '000124.csv' ---\n",
      "\n",
      "--- Processing '000125.csv' ---\n",
      "\n",
      "--- Processing '000126.csv' ---\n",
      "\n",
      "Data successfully saved as CSV at 'C:\\Users\\mydoa\\Desktop\\BIDFTA DATASET\\auctions-dataset\\tools\\nodejs-dataset-downloader\\02_filtered\\calculation5.csv'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os \n",
    "import pandas as pd\n",
    "\n",
    "# Define the path to the folder\n",
    "folder_path = r\"C:\\Users\\mydoa\\Desktop\\BIDFTA DATASET\\auctions-dataset\\tools\\nodejs-dataset-downloader\\02_filtered\\items\" #Defines folder_path, a string with the path to the folder containing files to be processed. The r before the string indicates a raw string to treat backslashes \\ as literal characters.\n",
    "auction_location_path = r\"C:\\Users\\mydoa\\Desktop\\BIDFTA DATASET\\auctions-dataset\\tools\\nodejs-dataset-downloader\\auctions-dataset-filtered-auctions\\auctions\\auctions.csv\"\n",
    "location_info_path = r\"C:\\Users\\mydoa\\Desktop\\BIDFTA DATASET\\auctions-dataset\\tools\\nodejs-dataset-downloader\\auctions-dataset-filtered-auctions\\auctions_data\\auctions_locations.csv\"\n",
    "pickupdates_path = r\"C:\\Users\\mydoa\\Desktop\\BIDFTA DATASET\\auctions-dataset\\tools\\nodejs-dataset-downloader\\auctions-dataset-filtered-auctions\\auctions_data\\auctions_pickupdates.csv\"\n",
    "\n",
    "# Initialize an empty list to store rows of item details directly\n",
    "data = []\n",
    "\n",
    "# Load the auctions file containing auction_id to location mapping\n",
    "try:\n",
    "    auctions_df = pd.read_csv(auction_location_path)\n",
    "    print(\"Loaded auctions file with auction_id to location mapping.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the auctions file: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "# Create a dictionary mapping auction_id (ID) to location_ID\n",
    "location_dict = {}\n",
    "try:\n",
    "    auctions_df = pd.read_csv(auction_location_path, delimiter='\\t', usecols=[\"ID\", \"location_ID\"])\n",
    "    auctions_df['ID'] = auctions_df['ID'].astype(str)  # Ensure ID is a string for consistency\n",
    "    \n",
    "    # Create a dictionary mapping auction_id (ID) to location_ID\n",
    "    location_dict = dict(zip(auctions_df['ID'], auctions_df['location_ID']))\n",
    "    location_dict\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the auctions file: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "# Create a dictionary for location details using location_ID as the key\n",
    "location_info_dict = {}\n",
    "try:\n",
    "    location_info_df = pd.read_csv(location_info_path, delimiter='\\t', usecols=[\"id\", \"state\", \"zip\",\"tzoffset_utc\",\"tzoffset_et\"])\n",
    "    location_info_df['id'] = location_info_df['id'].astype(str)  # Ensure location_ID (id) is a string\n",
    "    location_info_df['zip'] = location_info_df['zip'].astype(str)  # Ensure zip is stored as string\n",
    "    \n",
    "    # Populate location_info_dict with location_ID as key and (state, zip, tzoffset_utc, tzoffset_et) as values\n",
    "    location_info_dict = location_info_df.set_index('id')[['state', 'zip']].to_dict(orient='index')\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading location info file: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "#Create a dictionary for pickupdates as key and auction ID as value\n",
    "auctionsID_dict = {}\n",
    "try:\n",
    "    auctionsID_df = pd.read_csv(pickupdates_path, delimiter='\\t', usecols=[\"auction_ID\",\"date\"])\n",
    "    auctionsID_df['date'] = auctionsID_df['date'].astype(str)\n",
    "    \n",
    "    # Populate auctionsID_dict with pickupdates as key and a list of auction_ID as values\n",
    "    for _, row in auctionsID_df.iterrows():\n",
    "        auction_id = row['auction_ID']\n",
    "        pickupdate = row['date']\n",
    "        if pickupdate in auctionsID_dict:\n",
    "            auctionsID_dict[pickupdate].append(auction_id)\n",
    "        else:\n",
    "            auctionsID_dict[pickupdate] = [auction_id]\n",
    "    print(\"Created pickupdates dictionary for pickupdate to auction_ID mapping.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading pickupdates file: {e}\")\n",
    "\n",
    "\n",
    "# Create a dictionary for pickupdates using pickupdates_path\n",
    "pickupdates_dict = {}\n",
    "try:\n",
    "    pickupdates_df = pd.read_csv(pickupdates_path, delimiter='\\t', usecols=[\"auction_ID\", \"date\"])\n",
    "    pickupdates_df['auction_ID'] = pickupdates_df['auction_ID'].astype(str)  # Ensure auction_ID is a string\n",
    "    \n",
    "    # Populate pickupdates_dict with auction_ID as key and a list of pickupdate as values\n",
    "    for _, row in pickupdates_df.iterrows():\n",
    "        auction_id = row['auction_ID']\n",
    "        pickupdate = row['date']\n",
    "        if auction_id in pickupdates_dict:\n",
    "            pickupdates_dict[auction_id].append(pickupdate)\n",
    "        else:\n",
    "            pickupdates_dict[auction_id] = [pickupdate]\n",
    "    print(\"Created pickupdates dictionary for auction_ID to pickupdate mapping.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading pickupdates file: {e}\")\n",
    "\n",
    "\n",
    "# Create a nested dictionary for auction, location and pickupdates\n",
    "auction_location_pickupdates = {}\n",
    "try:\n",
    "    for auction_id, location_id in location_dict.items():\n",
    "        if auction_id in pickupdates_dict:\n",
    "            pickupdates = pickupdates_dict[auction_id]\n",
    "            if auction_id not in auction_location_pickupdates:\n",
    "                auction_location_pickupdates[auction_id] = {}\n",
    "            auction_location_pickupdates[auction_id][location_id] = pickupdates\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while building the nested dictionary: {e}\")\n",
    "\n",
    "\n",
    "# Initialize variables\n",
    "bundling_data = []\n",
    "pickup_dates = []\n",
    "filtered_auctions = []\n",
    "user_id_counts = {}\n",
    "\n",
    "# MAIN\n",
    "try:\n",
    "    files = os.listdir(folder_path)[:100]\n",
    "    for file_name in files:\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "        if os.path.isfile(file_path):\n",
    "            print(f\"\\n--- Processing '{file_name}' ---\")\n",
    "            try:\n",
    "                with open(file_path, 'r') as file:\n",
    "                    for line_number, line in enumerate(file, start=1):\n",
    "                        row_data = line.strip().split('\\t')\n",
    "                        if line_number == 1 or len(row_data) < 13:  # Skip header and invalid rows\n",
    "                            continue\n",
    "\n",
    "                        auction_id = row_data[0]\n",
    "                        item_id = row_data[1]\n",
    "                        user_id = row_data[12]\n",
    "\n",
    "                        if not user_id or user_id.lower() == 'none':\n",
    "                            continue  # Skip invalid user IDs\n",
    "\n",
    "\n",
    "                        # Retrieve pickup dates\n",
    "                        pickup_dates = []\n",
    "                        if auction_id in auction_location_pickupdates:\n",
    "                            for location_id, dates in auction_location_pickupdates[auction_id].items():\n",
    "                                pickup_dates.extend(dates)\n",
    "\n",
    "\n",
    "\n",
    "                        # Retrieve auctions with the same pickup date\n",
    "                        filtered_auctions = []\n",
    "                        for pickup_date in pickup_dates:\n",
    "                            same_pickup_auctions = auctionsID_dict.get(pickupdate, [])\n",
    "                            for same_auction_id in same_pickup_auctions:\n",
    "                                locations_auction_id = auction_location_pickupdates.get(auction_id, {}).keys()\n",
    "                                locations_same_auction_id = auction_location_pickupdates.get(same_auction_id, {}).keys()\n",
    "\n",
    "                                # Check location match\n",
    "                                if set(locations_auction_id) & set(locations_same_auction_id):\n",
    "                                    filtered_auctions.append(same_auction_id)\n",
    "\n",
    "\n",
    "                        # Process items in filtered auctions\n",
    "                        for loc_auction_id in filtered_auctions:\n",
    "                            same_auction_file_path = os.path.join(folder_path, f\"{loc_auction_id}.csv\")\n",
    "                            if os.path.isfile(same_auction_file_path):\n",
    "                                try:\n",
    "                                    same_auction_items_df = pd.read_csv(same_auction_file_path, delimiter='\\t')\n",
    "                                    same_auction_items_df['user_id'] = same_auction_items_df['user_id'].astype(str)\n",
    "\n",
    "                                    # Count occurrences of user IDs\n",
    "                                    for _, item_row in same_auction_items_df.iterrows():\n",
    "                                        current_user_id = item_row['user_id']\n",
    "                                        current_item_id = item_row['item_id']\n",
    "\n",
    "                                        if current_user_id in user_id_counts:\n",
    "                                            user_id_counts[current_user_id] += 1\n",
    "                                        else:\n",
    "                                            user_id_counts[current_user_id] = 1\n",
    "                                            \n",
    "                                            \n",
    "                                            \n",
    "                                            \n",
    "                                            \n",
    "                                            bundling_data.append([loc_auction_id, current_item_id,current_user_id])\n",
    "\n",
    "                                except Exception as e:\n",
    "                                    print(f\"Error processing items for auction {loc_auction_id}: {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred while reading '{file_name}': {e}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"The folder '{folder_path}' was not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Convert bundling data to a DataFrame\n",
    "df = pd.DataFrame(bundling_data, columns=[\"location_id\", \"current_item_id\", \"current_user_id\"])\n",
    "bundling_df = pd.DataFrame(bundling_data)\n",
    "\n",
    "# Define the path for saving the output as CSV\n",
    "csv_path = r\"C:\\Users\\mydoa\\Desktop\\BIDFTA DATASET\\auctions-dataset\\tools\\nodejs-dataset-downloader\\02_filtered\\calculation5.csv\"\n",
    "try:\n",
    "    bundling_df.to_csv(csv_path, index=False)  # Save as CSV without the index\n",
    "    print(f\"\\nData successfully saved as CSV at '{csv_path}'\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while saving CSV: {e}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2018-09-27', '2018-09-28']\n"
     ]
    }
   ],
   "source": [
    "print(pickup_dates)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
