{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os \n",
    "import pandas as pd\n",
    "\n",
    "# Define the path to the folder\n",
    "folder_path = r\"C:\\Users\\mydoa\\Desktop\\BIDFTA DATASET\\auctions-dataset\\tools\\nodejs-dataset-downloader\\02_filtered\\items\" #Defines folder_path, a string with the path to the folder containing files to be processed. The r before the string indicates a raw string to treat backslashes \\ as literal characters.\n",
    "auction_location_path = r\"C:\\Users\\mydoa\\Desktop\\BIDFTA DATASET\\auctions-dataset\\tools\\nodejs-dataset-downloader\\auctions-dataset-filtered-auctions\\auctions\\auctions.csv\"\n",
    "location_info_path = r\"C:\\Users\\mydoa\\Desktop\\BIDFTA DATASET\\auctions-dataset\\tools\\nodejs-dataset-downloader\\auctions-dataset-filtered-auctions\\auctions_data\\auctions_locations.csv\"\n",
    "pickupdates_path = r\"C:\\Users\\mydoa\\Desktop\\BIDFTA DATASET\\auctions-dataset\\tools\\nodejs-dataset-downloader\\auctions-dataset-filtered-auctions\\auctions_data\\auctions_pickupdates.csv\"\n",
    "\n",
    "# Initialize an empty list to store rows of item details directly\n",
    "data = []\n",
    "\n",
    "# Load the auctions file containing auction_id to location mapping\n",
    "try:\n",
    "    auctions_df = pd.read_csv(auction_location_path)\n",
    "    print(\"Loaded auctions file with auction_id to location mapping.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the auctions file: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "# Create a dictionary mapping auction_id (ID) to location_ID\n",
    "location_dict = {}\n",
    "try:\n",
    "    auctions_df = pd.read_csv(auction_location_path, delimiter='\\t', usecols=[\"ID\", \"location_ID\"])\n",
    "    auctions_df['ID'] = auctions_df['ID'].astype(str)  # Ensure ID is a string for consistency\n",
    "    \n",
    "    # Create a dictionary mapping auction_id (ID) to location_ID\n",
    "    location_dict = dict(zip(auctions_df['ID'], auctions_df['location_ID']))\n",
    "    location_dict\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the auctions file: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "# Create a dictionary for location details using location_ID as the key\n",
    "location_info_dict = {}\n",
    "try:\n",
    "    location_info_df = pd.read_csv(location_info_path, delimiter='\\t', usecols=[\"id\", \"state\", \"zip\",\"tzoffset_utc\",\"tzoffset_et\"])\n",
    "    location_info_df['id'] = location_info_df['id'].astype(str)  # Ensure location_ID (id) is a string\n",
    "    location_info_df['zip'] = location_info_df['zip'].astype(str)  # Ensure zip is stored as string\n",
    "    \n",
    "    # Populate location_info_dict with location_ID as key and (state, zip, tzoffset_utc, tzoffset_et) as values\n",
    "    location_info_dict = location_info_df.set_index('id')[['state', 'zip']].to_dict(orient='index')\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading location info file: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "#Create a dictionary for pickupdates as key and auction ID as value\n",
    "auctionsID_dict = {}\n",
    "try:\n",
    "    auctionsID_df = pd.read_csv(pickupdates_path, delimiter='\\t', usecols=[\"auction_ID\",\"date\"])\n",
    "    auctionsID_df['date'] = auctionsID_df['date'].astype(str)\n",
    "    \n",
    "    # Populate auctionsID_dict with pickupdates as key and a list of auction_ID as values\n",
    "    for _, row in auctionsID_df.iterrows():\n",
    "        auction_id = row['auction_ID']\n",
    "        pickupdate = row['date']\n",
    "        if pickupdate in auctionsID_dict:\n",
    "            auctionsID_dict[pickupdate].append(auction_id)\n",
    "        else:\n",
    "            auctionsID_dict[pickupdate] = [auction_id]\n",
    "    print(\"Created pickupdates dictionary for pickupdate to auction_ID mapping.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading pickupdates file: {e}\")\n",
    "\n",
    "\n",
    "# Create a dictionary for pickupdates using pickupdates_path\n",
    "pickupdates_dict = {}\n",
    "try:\n",
    "    pickupdates_df = pd.read_csv(pickupdates_path, delimiter='\\t', usecols=[\"auction_ID\", \"date\"])\n",
    "    pickupdates_df['auction_ID'] = pickupdates_df['auction_ID'].astype(str)  # Ensure auction_ID is a string\n",
    "    \n",
    "    # Populate pickupdates_dict with auction_ID as key and a list of pickupdate as values\n",
    "    for _, row in pickupdates_df.iterrows():\n",
    "        auction_id = row['auction_ID']\n",
    "        pickupdate = row['date']\n",
    "        if auction_id in pickupdates_dict:\n",
    "            pickupdates_dict[auction_id].append(pickupdate)\n",
    "        else:\n",
    "            pickupdates_dict[auction_id] = [pickupdate]\n",
    "    print(\"Created pickupdates dictionary for auction_ID to pickupdate mapping.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading pickupdates file: {e}\")\n",
    "\n",
    "\n",
    "# Create a nested dictionary for auction, location and pickupdates\n",
    "auction_location_pickupdates = {}\n",
    "try:\n",
    "    for auction_id, location_id in location_dict.items():\n",
    "        if auction_id in pickupdates_dict:\n",
    "            pickupdates = pickupdates_dict[auction_id]\n",
    "            if auction_id not in auction_location_pickupdates:\n",
    "                auction_location_pickupdates[auction_id] = {}\n",
    "            auction_location_pickupdates[auction_id][location_id] = pickupdates\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while building the nested dictionary: {e}\")\n",
    "\n",
    "\n",
    "# Initialize variables\n",
    "bundling_data = []\n",
    "pickup_dates = []\n",
    "filtered_auctions = []\n",
    "user_id_counts = {}\n",
    "\n",
    "# MAIN\n",
    "try:\n",
    "    files = os.listdir(folder_path)[:100]\n",
    "    for file_name in files:\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "        if os.path.isfile(file_path):\n",
    "            print(f\"\\n--- Processing '{file_name}' ---\")\n",
    "            try:\n",
    "                with open(file_path, 'r') as file:\n",
    "                    for line_number, line in enumerate(file, start=1):\n",
    "                        row_data = line.strip().split('\\t')\n",
    "                        if line_number == 1 or len(row_data) < 13:  # Skip header and invalid rows\n",
    "                            continue\n",
    "\n",
    "                        auction_id = row_data[0]\n",
    "                        item_id = row_data[1]\n",
    "                        user_id = row_data[12]\n",
    "\n",
    "                        if not user_id or user_id.lower() == 'none':\n",
    "                            continue  # Skip invalid user IDs\n",
    "\n",
    "\n",
    "                        # Retrieve pickup dates\n",
    "                        pickup_dates = []\n",
    "                        if auction_id in auction_location_pickupdates:\n",
    "                            for location_id, dates in auction_location_pickupdates[auction_id].items():\n",
    "                                pickup_dates.extend(dates)\n",
    "\n",
    "\n",
    "\n",
    "                        # Retrieve auctions with the same pickup date\n",
    "                        filtered_auctions = []\n",
    "                        for pickup_date in pickup_dates:\n",
    "                            same_pickup_auctions = auctionsID_dict.get(pickup_date, [])\n",
    "                            for same_auction_id in same_pickup_auctions:\n",
    "                                locations_auction_id = auction_location_pickupdates.get(auction_id, {}).keys()\n",
    "                                locations_same_auction_id = auction_location_pickupdates.get(same_auction_id, {}).keys()\n",
    "\n",
    "                                # Check location match\n",
    "                                if set(locations_auction_id) & set(locations_same_auction_id):\n",
    "                                    filtered_auctions.append(same_auction_id)\n",
    "\n",
    "\n",
    "                        # Process items in filtered auctions\n",
    "                        for loc_auction_id in filtered_auctions:\n",
    "                            same_auction_file_path = os.path.join(folder_path, f\"{loc_auction_id}.csv\")\n",
    "                            if os.path.isfile(same_auction_file_path):\n",
    "                                try:\n",
    "                                    same_auction_items_df = pd.read_csv(same_auction_file_path, delimiter='\\t')\n",
    "                                    same_auction_items_df['user_id'] = same_auction_items_df['user_id'].astype(str)\n",
    "\n",
    "                                    # Count occurrences of user IDs\n",
    "                                    for _, item_row in same_auction_items_df.iterrows():\n",
    "                                        current_user_id = item_row['user_id']\n",
    "                                        current_item_id = item_row['item_id']\n",
    "\n",
    "                                        if current_user_id in user_id_counts:\n",
    "                                            user_id_counts[current_user_id] += 1\n",
    "                                        else:\n",
    "                                            user_id_counts[current_user_id] = 1\n",
    "                                            bundling_data.append({\n",
    "                                                \"auction_ID\": loc_auction_id,\n",
    "                                                \"item_ID\": current_item_id,\n",
    "                                                \"user_ID\": current_user_id,\n",
    "                                                \"bundling_count\": 1\n",
    "                                            })\n",
    "\n",
    "                                except Exception as e:\n",
    "                                    print(f\"Error processing items for auction {loc_auction_id}: {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred while reading '{file_name}': {e}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"The folder '{folder_path}' was not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Convert bundling data to a DataFrame\n",
    "bundling_df = pd.DataFrame(bundling_data)\n",
    "\n",
    "# Define the path for saving the output as CSV\n",
    "csv_path = r\"C:\\Users\\mydoa\\Desktop\\BIDFTA DATASET\\auctions-dataset\\tools\\nodejs-dataset-downloader\\02_filtered\\calculation5.csv\"\n",
    "try:\n",
    "    bundling_df.to_csv(csv_path, index=False)  # Save as CSV without the index\n",
    "    print(f\"\\nData successfully saved as CSV at '{csv_path}'\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while saving CSV: {e}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
